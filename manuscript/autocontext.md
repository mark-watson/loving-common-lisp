# AutoContext: Prepare Effective Prompts with Context for LLM Queries

Dear reader, given a large corpus of text documents, and given a query, how do we create a combined one-shot prompt that contains a relatively small context to include in the prompt?

We start by processing our large text corpus into small two or three sentence “chunks.” We combine BM25 (lexical search) and vector similarity (semantic search) into hybrid search that is used to identify, given a user’s question, a small number of chunks that we use to construct a one-shot prompt with this small tailored context and the user’s original question.

The example in this chapter uses the **magicl** library that is tested to run on SBCL.

The purpose of this example is allowing the use of small models supporting small contexts and still take advantage of huge text datasets.


## Implementing the BM25 Algorithm

The following Common Lisp code in **bm25.lisp** provides a complete self-contained implementation of the Okapi BM25 ranking function, a powerful algorithm widely used in information retrieval and search engine technology. Unlike simpler term frequency-inverse document frequency (TF-IDF) models, BM25 is a probabilistic model that scores the relevance of documents to a given search query by considering not only the frequency of query terms within a document but also the document's length relative to the average length in the entire corpus. This allows it to penalize overly long documents and account for term saturation, where the relevance score doesn't increase proportionally after a term appears a certain number of times. This implementation encapsulates the logic within a **bm25-index** class, which pre-calculates and stores necessary statistics about the text corpus such as document frequencies, document lengths, and the average document length, to enable efficient scoring and retrieval of the most relevant text chunks from documents for a user's query. In our application we use this code to process individual small text chunks and not entire documents. I used Google Gemini cons gemini-cli to write and debug this code:

```lisp
;; bm25.lisp NOTE: Generated by Gemini 2.5 Pro in Research Mode
;; and corrected by gemini-cli.
;;
;; A self-contained implementation of the BM25Okapi ranking function.

(defpackage #:bm25
  (:use #:cl)
  (:export #:bm25-index
           #:make-bm25-index
           #:get-top-n))

(in-package #:bm25)

(defclass bm25-index ()
  ((doc-freqs :initarg :doc-freqs :reader doc-freqs)
   (doc-lengths :initarg :doc-lengths :reader doc-lengths)
   (avg-doc-length :initarg :avg-doc-length :reader avg-doc-length)
   (corpus-size :initarg :corpus-size :reader corpus-size)
   (corpus :initarg :corpus :reader corpus)
   (k1 :initarg :k1 :reader k1)
   (b :initarg :b :reader b)))

(defun make-bm25-index (tokenized-corpus &key (k1 1.5) (b 0.75))
  "Creates and initializes a BM25 index from a corpus of tokenized documents."
  (let* ((corpus-size (length tokenized-corpus))
         (doc-lengths (mapcar #'length tokenized-corpus))
         (avg-doc-length (/ (reduce #'+ doc-lengths) corpus-size))
         (doc-freqs (make-hash-table :test 'equal)))
    ;; Calculate document frequencies for each term
    (dolist (doc tokenized-corpus)
      (dolist (term (remove-duplicates doc :test #'string=))
        (incf (gethash term doc-freqs 0))))
    (make-instance 'bm25-index
                   :doc-freqs doc-freqs
                   :doc-lengths doc-lengths
                   :avg-doc-length avg-doc-length
                   :corpus-size corpus-size
                   :corpus tokenized-corpus
                   :k1 k1
                   :b b)))

(defmethod get-top-n ((index bm25-index) query-tokens n)
  "Returns the top N documents from the corpus for a given query."
  (let ((scores (loop for doc in (corpus index)
                      for i from 0
                      collect (cons (score-doc index query-tokens i) i))))
    ;; Sort by score descending
    (let* ((sorted-scores (sort scores #'> :key #'car))
           (top-scores (subseq sorted-scores 0 (min n (length sorted-scores)))))
      ;; Return the original documents, not the tokenized versions
      (mapcar (lambda (score-pair)
                (nth (cdr score-pair) (slot-value index 'corpus)))
              top-scores))))

;;; Internal methods

(defmethod inverse-document-frequency ((index bm25-index) term)
  "Calculates the IDF for a given term."
  (let* ((doc-freq (gethash term (doc-freqs index) 0))
         (corpus-size (corpus-size index)))
    (log (/ (+ (- corpus-size doc-freq) 0.5) (+ doc-freq 0.5)) 10)))

(defmethod score-doc ((index bm25-index) query-tokens doc-index)
  "Calculates the BM25 score for a single document."
  (let* ((k1 (k1 index))
         (b (b index))
         (doc-length (nth doc-index (doc-lengths index)))
         (doc (nth doc-index (corpus index)))
         (avg-dl (avg-doc-length index))
         (score 0.0))
    (dolist (term query-tokens)
      (let* ((term-freq (count term doc :test #'string=))
             (idf (inverse-document-frequency index term)))
        (incf score (* idf (/ (* term-freq (+ k1 1))
                               (+ term-freq
                                  (* k1
                                     (+ (- 1 b)
                                        (* b
                                           (/
                                             doc-length
                                             avg-dl))))))))))
    score))
```

The constructor function, **make-bm25-index**, handles the heavy lifting of tokenizing the corpus, calculating the document frequency for every term, and determining the length of each document. This upfront processing makes subsequent searches very efficient. The primary public-facing function, **get-top-n** takes an index, a list of query tokens, and the desired number of results **n**. It scores every document against the query, sorts them in descending order of relevance, and returns the top **n** matching documents, providing a simple and effective interface for search.

The core of the relevance calculation is found in the **score-doc** method, that implements the BM25 formula. For each term in the user's query, it calculates a score based on three main components: the inverse document frequency (IDF), the term's frequency within the specific document, and the length of that document. The helper method **inverse-document-frequency** computes the IDF, which gives higher weight to rarer terms across the corpus. The main **score-doc** method combines this with a term frequency component that is tunable via the **k1** and **b** parameters. The **k1** parameter controls term frequency saturation, while **b** controls the degree to which document length normalizes the score, making this a flexible and powerful tool for document ranking.

## Implementing Vectorization of Text and Semantic Similarity

We will take advantage of Python’s library support for deep learning and use a Python command line tool **generate_embeddings.py** to calculate vector embeddings for text input using a deep learning transformer model and returns the results in JSON format. Here is an example of calling this script manually:

```bash
echo "some text" | uv run generate_embeddings.py
```

This script will be called from our Common Lisp code.

For completeness we list the Python script without comments:

```python
# generate_embeddings.py
#
# This script reads text lines from stdin, generates embeddings using
# sentence-transformers, and prints the result as a JSON array to stdout.
# This allows the Common Lisp program to easily get embeddings without
# needing a complex foreign function interface.
#
# Usage (from Lisp):
# echo "some text" | python3 generate_embeddings.py

import sys
import json
from sentence_transformers import SentenceTransformer

def main():
    # Use a pre-trained model, which will be downloaded on first run.
    model_name = 'all-MiniLM-L6-v2'
    try:
        model = SentenceTransformer(model_name)
    except Exception as e:
        print(f"Error loading SentenceTransformer model: {e}", file=sys.stderr)
        sys.exit(1)

    # Read all lines from standard input.
    lines = [line.strip() for line in sys.stdin if line.strip()]

    if not lines:
        print("[]") # Output empty JSON array if no input
        return

    # Generate embeddings and normalize them for cosine similarity.
    embeddings = model.encode(
        lines,
        normalize_embeddings=True,
        show_progress_bar=False # Keep stdout clean for the Lisp process
    )

    # Convert the numpy array to a list of lists for JSON serialization.
    embeddings_list = embeddings.tolist()

    # Print the JSON output to stdout.
    json.dump(embeddings_list, sys.stdout)

if __name__ == '__main__':
    main()
```

The following function runs the Python script and is found in the file **main.lisp** that we discuss in the next section.

```lisp
(defun generate-embeddings (text-list)
  "Calls the Python script to generate embeddings for a list of strings."
  (let* ((input-string (format nil "~{~a~%~}" text-list))
         (command "uv run generate_embeddings.py")
         (json-output (uiop:run-program command
                                        :input (make-string-input-stream input-string)
                                        :output :string)))
    (let* ((parsed (yason:parse json-output))
           (num-embeddings (length parsed))
           (embedding-dim (if (> num-embeddings 0) (length (first parsed)) 0))
           (flat-data (apply #'append parsed)))
      (magicl:from-list
        flat-data
        (list num-embeddings embedding-dim) :type 'double-float))))
```

## Implementation of Main Program

This listing shows **main.lisp**, the core of our hybrid Retrieval-Augmented Generation (RAG) system written in Common Lisp that also constructs a one-shot prompt for later input into a LLM.

The **auto-context** class manages the entire lifecycle of loading, processing, and querying a local document collection. On instantiation, it scans a directory for text files, breaks them down into smaller, semantically coherent chunks, and then builds two parallel retrieval indices. The first is a classic BM25 sparse index for efficient keyword-based search, and the second is a dense index of vector embeddings for capturing semantic similarity. A key feature of this implementation is its pragmatic approach to interoperability; it generates embeddings by calling an external Python script that leverages the sentence-transformers library, communicating via standard I/O and JSON. The primary entry point for users is the **get-prompt** method, which takes a query, performs searches against both indices, merges the results into a unified context, and formats a complete prompt ready to be processed by a Large Language Model.

```lisp
;; main.lisp
;; The core AutoContext class and application logic.

(defpackage #:autocontext
  (:use #:cl #:bm25)
  (:export #:auto-context
           #:get-prompt
           #:run-example))

(in-package #:autocontext)

(defclass auto-context ()
  ((chunks :reader chunks :initarg :chunks
           :documentation "A list of original text chunks.")
   (bm25-index :reader bm25-index :initarg :bm25-index
               :documentation "The BM25 sparse index.")
   (chunk-embeddings :reader chunk-embeddings :initarg :chunk-embeddings
                     :documentation "A magicl matrix of dense embeddings.")))

;;; Initialization

(defmethod initialize-instance :after ((ac auto-context) &key directory-path)
  "Constructor for the auto-context class. Loads data and builds indices."
  (format t "~&Initializing AutoContext from directory: ~a" directory-path)
  (let ((chunks (load-and-chunk-documents directory-path)))
    (when chunks
      (format t "~&Building sparse and dense retrievers...")
      (let* ((tokenized-chunks (mapcar #'tokenize chunks))
             (bm25 (make-bm25-index tokenized-chunks))
             (embeddings (generate-embeddings chunks)))
        ;; Using shared-initialize to set the slots after creation
        (shared-initialize ac t :chunks chunks
                               :bm25-index bm25
                               :chunk-embeddings embeddings))
      (format t "~&Initialization complete. AutoContext is ready."))))

(defun tokenize (text)
  "Simple whitespace tokenizer."
  (split-sequence:split-sequence #\Space text :remove-empty-subseqs t))

(defun list-txt-files-in-directory (dir-path)
  "Return a list of pathnames for *.txt files in the directory given by DIR-PATH (a string or pathname)."
  (let* ((base (uiop:parse-native-namestring dir-path))
         ;; ensure base is a directory pathname
         (dir (uiop:ensure-directory-pathname base))
         ;; create a wildcard pathname for *.txt under that directory
         (pattern (uiop:merge-pathnames* 
                    (make-pathname :name :wild :type "txt")
                    dir)))
    (directory pattern)))


(defun split-into-sentences (text)
  "Splits text into a list of sentences based on punctuation. This is a heuristic approach and may not be perfect. It tries to avoid splitting on abbreviations like 'e.g.'."
  (let ((sentences '())
        (start 0))
    (loop for i from 0 below (length text)
          do (when (and (member (char text i) '(#\. #\? #\!))
                        (or (= (1+ i) (length text))
                            (member (char text (1+ i)) '(#\Space #\Newline #\"))))
               (push
                 (string-trim '(#\Space #\Newline) (subseq text start (1+ i)))
                  sentences)
               (setf start (1+ i))))
    (let ((last-part (string-trim '(#\Space #\Newline) (subseq text start))))
      (when (plusp (length last-part))
        (push last-part sentences)))
    (remove-if (lambda (s) (zerop (length s))) (nreverse sentences))))

(defun chunk-text (text &key (chunk-size 3))
  "Splits text into sentences and then groups them into chunks of chunk-size sentences."
  (let ((sentences (split-into-sentences text)))
    (loop for i from 0 below (length sentences) by chunk-size
          collect (let* ((end (min (+ i chunk-size) (length sentences)))
                         (sentence-group (subseq sentences i end)))
                    (string-trim
                      '(#\Space #\Newline)
                      (format nil "~{~a~^ ~}" sentence-group))))))


(defun load-and-chunk-documents (directory-path)
  "Loads .txt files and splits them into chunks of a few sentences."
  ;;(format t "&load-and-chunk-documents: directory-path=~A~%" directory-path)
  (let* ((chunks (remove-if (lambda (s) (string= s ""))
                            (loop for file in
                                   (list-txt-files-in-directory directory-path)
                                  nconcing (chunk-text (uiop:read-file-string file))))))
    (format t "~&Loaded ~d text chunks." (length chunks))
    ;;(format t "&load-and-chunk-documents: chunks=~A~%~%" chunks)
    chunks));;; Embedding Generation (Interface to Python)

(defun generate-embeddings (text-list)
  "Calls the Python script to generate embeddings for a list of strings."
  (let* ((input-string (format nil "~{~a~%~}" text-list))
         (command "uv run generate_embeddings.py")
         (json-output (uiop:run-program command
                                        :input
                                        (make-string-input-stream input-string)
                                        :output :string)))
    (let* ((parsed (yason:parse json-output))
           (num-embeddings (length parsed))
           (embedding-dim (if (> num-embeddings 0) (length (first parsed)) 0))
           (flat-data (apply #'append parsed)))
      (magicl:from-list
        flat-data
        (list num-embeddings embedding-dim) :type 'double-float))))


(defun cosine-similarity (vec1 vec2)
  "Calculates cosine similarity between two magicl vectors."
  (/ (magicl:dot vec1 vec2)
     (* (magicl:norm vec1) (magicl:norm vec2))))

(defun get-row-vector (matrix row-index)
  "Extracts a row from a matrix and returns it as a magicl vector."
  (let* ((num-cols (magicl:ncols matrix))
         (row-elements (loop for col-index from 0 below num-cols
                             collect (magicl:tref matrix row-index col-index))))
    (magicl:from-list
      row-elements (list num-cols)
      :type (magicl:element-type matrix))))

(defmethod get-prompt ((ac auto-context) query &key (num-results 5))
  "Retrieves context and formats it into a prompt for an LLM."
  (format t "~&--- Retrieving context for query: '~a' ---" query)

  ;; 1. Sparse Search (BM25)
  (let* ((query-tokens (tokenize query))
         (bm25-docs (bm25:get-top-n (bm25-index ac) query-tokens num-results))
         (bm25-results
           (mapcar (lambda (tokens)
                     (format nil "~{~a~^ ~}" tokens)) bm25-docs)))
    (format t "~&BM25 found ~d keyword-based results." (length bm25-results))
    ;;(format t "~%~%bm25-results:~%~A~%~%" bm25-results)

     ;; 2. Dense Search (Vector Similarity)
     (let* ((query-embedding-matrix (generate-embeddings (list query)))
            (query-vector (get-row-vector query-embedding-matrix 0))
            (all-embeddings (chunk-embeddings ac))
            (similarities
              (loop for i from 0 below (magicl:nrows all-embeddings)
                collect (cons (cosine-similarity query-vector
                                get-row-vector all-embeddings i))
                         i)))
            (sorted-sim (sort similarities #'> :key #'car))
            (top-indices
             (mapcar
               #'cdr
               (subseq sorted-sim 0
                       (min num-results (length sorted-sim)))))
            (vector-results
              (mapcar
                (lambda (i)
                  (nth i (chunks ac))) top-indices)))
       (format t "~&Vector search found ~d semantic-based results."
               (length vector-results))
       ;;(format t "~%~%vector-results:~%~A~%~%" vector-results)

       ;; 3. Combine and deduplicate
       (let* ((combined (append bm25-results vector-results))
              (unique-results
                (remove-duplicates combined
                  :test #'string= :from-end t)))
         (format t "~&Combined and deduplicated, we have ~d context chunks." (length unique-results))

         ;; 4. Format the final prompt
         (format nil "Based on the following context, please answer the question:~%~A~2%--- CONTEXT ---~%~{~a~^~%---~%~}--- END CONTEXT ---~2%Question: ~a~%Answer:"
                 query unique-results query)))))

;;; Example Usage

(defun test2 ()
  "A simple top-level function to demonstrate the system."
  (let* ((ac (make-instance 'auto-context :directory-path "../data"))
         (query "who says that economics is bullshit?")
         (prompt (get-prompt ac query :num-results 2)))
      (format t "~&~%--- Generated Prompt for LLM ---~%~a" prompt)))
```

This code demonstrates a powerful hybrid search technique. By leveraging both BM25 and vector similarity the retrieval process becomes more robust. BM25 is adept at finding chunks containing specific keywords or jargon present in the query, while the dense vector search excels at uncovering conceptually related content even when the exact wording differs. This dual approach mitigates the weaknesses of each individual method, leading to create a more comprehensive and relevant context for a LLM. The auto-context class neatly encapsulates this complexity, holding the text chunks, the sparse index, and the dense embedding matrix as a single cohesive unit.

From an implementation perspective the most notable detail is the seamless integration with Python for embedding generation. The generate-embeddings function wisely avoids the complexity of using a foreign function interface or trying to reimplement a transformer model in Lisp. Instead, it uses the simple and reliable method of running a command-line script and parsing its JSON output, a practical pattern for leveraging the strengths of different language ecosystems. Internally, the use of the **magicl** library to represent the embeddings as a matrix is also significant, as it provides an efficient, specialized data structure for the numerical calculations required for computing cosine similarity between the query and document vectors.

## Example Generated Prompt with Context

Here we run the **test2** function defined at the bottom of the last code listing (some output removed for brevity):

```text
$ sbcl
This is SBCL 2.5.3, an implementation of ANSI Common Lisp.
* (ql :autocontext)
To load "autocontext":
  Load 1 ASDF system:
    autocontext
; Loading "autocontext"
..................................................
[package bm25]....................................
[package autocontext]..

* (autocontext::test2)
Initializing AutoContext from directory: ../data
Loaded 22 text chunks.
Building sparse and dense retrievers...
Initialization complete. AutoContext is ready.
--- Retrieving context for query: 'who says that economics is bullshit?' ---
BM25 found 2 keyword-based results.
Vector search found 2 semantic-based results.
Combined and deduplicated, we have 4 context chunks.

--- Generated Prompt for LLM ---
Based on the following context, please answer the question:
who says that economics is bullshit?

--- CONTEXT ---

There exists an economic problem, subject to study by economic science, when a decision (choice) is made by one or more resource-controlling players to attain the best possible outcome under bounded rational conditions. In other words, resource-controlling agents maximize value subject to the constraints imposed by the information the agents have, their cognitive limitations, and the finite amount of time they have to make and execute a decision. Economic science centers on the activities of the economic agents that comprise society.[1] They are the focus of economic analysis.[2]
---
An interesting Economist is Pauli Blendergast who teaches at the University of Krampton Ohio and is famouse for saying economics is bullshit.
---
; Looking better can help you feel better. Schedule a realistic day. ; Avoid the tendency to schedule back-to-back appointments; allow time between appointments for a breathing spell.
---
    which requires that you sit at a desk all day. ; If you hate to talk
     politics, don't associate with people who love to talk politics, etc. Learn to live one day at a time.
 --- END CONTEXT ---

Question: who says that economics is bullshit?
Answer:
* 
```

Normally you would combine the code developed in this chapter in an application using a a small LLM running on your laptop using Ollama or LNM Studio, or huge LLMs like Gemini-2.5-pro or GPT-5 via commercial APIs.

Here, I feed the generated prompt into a tiny 3B model running on Ollama for test purposes (most output not shown):

```text
$ ollama run gemma3:latest
>>> Based on the following context, please answer the question:
... who says that economics is bullshit?
... 
... --- CONTEXT ---
... 
...  which requires that you sit at a desk all day. ; If you hate to talk
...  politics, don't associate with people who love to talk politics, etc. Learn to live one day at a time.
... ---
... An interesting Economist is Pauli Blendergast who teaches at the University of Krampton Ohio and is famouse fo
... r saying economics is bullshit.
... ---
... ; Looking better can help you feel better. Schedule a realistic day. ; Avoid the tendency to schedule back-to-
... back appointments; allow time between appointments for a breathing spell.
... ---
...  END CONTEXT ---
... 
... Question: who says that economics is bullshit?
... Answer:
Pauli Blendergast

>>>
```

## Wrap Up For Generating Prompts with Contexts

Large context models like Gemini 2.5-pro have a context window of a million tokens so in principle many large documents can be used as-is for context to a prompt.

My motivate for writing this example code is my desire to mostly write applications using smaller LLMs running locally using Ollama or LM Studio. Often these small models only support a context size from 16K to 64K in size and they run slowly on my laptop when processing very long one-shot prompts.

I find that example we developed here allows me to use small models and still take advantage of huge text datasets for context.

